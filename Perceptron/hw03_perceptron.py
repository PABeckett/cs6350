# -*- coding: utf-8 -*-
"""hw03_perceptron

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TiD1rEMn13tBskJ9bY-kTD7iHnBRu7jg
"""

#Parker Beckett u0283152
#HW03 perceptron
#!pip install scikit-learn
import math
import pandas as pd
import numpy as np
import scipy as sp
import sys #for arguments from bash
import sklearn as skl
from sklearn.tree import DecisionTreeClassifier
import random
from random import randrange
import warnings
warnings.filterwarnings("ignore")
#import warnings
#warnings.filterwarnings('ignore')
#import time #for benchmarking

#I/O file to add header line and load into dataframe
def load_csv(filename):

  attributecount = 0
  examplecount = 0

  with open(filename) as f:
    for line in f:
      examplecount += 1
      terms = line.strip().split(',')
      attributecount = len(terms)

  attributecount -= 1
  names = []

  for i in range(0,attributecount):
    name = "A"+str(i)
    names.append(name)

  names.append("label")

  df = pd.read_csv(filename, names = names)

  #so its [-1, 1]
  df.label.replace(0, -1, inplace = True)

  return df

def sgn(val):
  if val > 0:
    return 1
  else:
    return -1

def s_perc(data, r, T):

  weight = np.zeros(len(data.columns))
  print("starting perceptron - standard")
  for i in range(T):
    data = data.sample(frac = 1) #shuffle the data
    for index, row in data.iterrows():
      sample = row.to_numpy()
      x = sample[:-1]
      x = np.append(x, 1) #constant bias feature
      wtxi = np.dot(weight, x)
      yi = sample[len(sample) - 1]

      if yi*wtxi <= 0: #mistake
        weight = weight + r*yi*x

  print("finished perceptron - standard")

  return weight

def v_perc(data, r, T):

  warr = []
  carr = []

  warr.append(np.zeros(len(data.columns)))
  carr.append(0) #?

  m = 0

  print("starting perceptron - voted")
  #voted perceptron
  for i in range(T):
      data = data.sample(frac = 1) #shuffle the data
      for index, row in data.iterrows():
        sample = row.to_numpy()
        x = sample[:-1]
        x = np.append(x, 1) #constant bias feature

        wtxi = np.dot(warr[m], x)
        yi = sample[len(sample) - 1]

        if yi*wtxi <= 0: #mistake
          wnew = warr[m] + r*yi*x
          warr.append(wnew) #??
          m += 1
          carr.append(1)
        else:
          carr[m] += 1

  print("finished perceptron - voted")

  return warr, carr

def a_perc(data, r, T):
  weight = np.zeros(len(data.columns))
  aarr = np.zeros(len(data.columns))

  print("starting perceptron")
  #average perceptron
  for i in range(T):
    data = data.sample(frac = 1) #shuffle the data
    for index, row in data.iterrows():
      sample = row.to_numpy()
      x = sample[:-1]
      x = np.append(x, 1) #constant bias feature
      wtxi = np.dot(weight, x)
      yi = sample[len(sample) - 1]

      if yi*wtxi <= 0: #mistake
        weight = weight + r*yi*x
    aarr = aarr + weight
  print("finished perceptron")

  return aarr

def stest(data, weight, mode):
  print(data.head)
  num = data.shape[0]
  hit = 0
  for index, row in data.iterrows():
    sample = row.to_numpy()
    y = sample[len(sample) - 1]
    x = sample[:-1]
    x = np.append(x, 1)

    pred = sgn(np.dot(weight, x))

    if pred == y:
      hit += 1

  print(hit, "/", num)
  return (hit / num)

def vtest(data, warr, carr, mode):
  num = data.shape[0]
  hit = 0
  for index, row in data.iterrows():
    sample = row.to_numpy()
    y = sample[len(sample) - 1]
    x = sample[:-1]
    x = np.append(x, 1)

    sum = 0

    for i in range(len(warr)):
      sum += carr[i] * sgn(np.dot(warr[i], x))

    pred = sgn(sum)

    if pred == y:
      hit += 1

  print(hit, "/", num)
  return (hit / num)

def atest(data, aarr, mode):
  num = data.shape[0]
  hit = 0
  for index, row in data.iterrows():
    sample = row.to_numpy()
    y = sample[len(sample) - 1]
    x = sample[:-1]
    x = np.append(x, 1)

    pred = sgn(np.dot(aarr, x))

    if pred == y:
      hit += 1

  print(hit, "/", num)
  return (hit / num)

def main():

  print("main")

  modearg = str(sys.argv[1])
  rarg = int(sys.argcv[2])
  earg = int(sys.argv[3])
  trainarg = str(sys.argv[4])
  testarg = str(sys.argv[5])

  #mode can be s, v, or a
  #modearg = 'a'
  #rarg = .5
  #earg = 10
  #trainarg = "train.csv"
  #testarg = "test.csv"

  trdata = load_csv(trainarg)
  ttdata = load_csv(testarg)

  err = 0

  if modearg == "st":
    weight = s_perc(trdata, rarg, earg)
    err = stest(ttdata, weight, modearg)
    print("standard: w = [", weight, "], \n e = ", err, " for ", earg, " epochs and " , rarg, " learning rate")

  if modearg == "vo":
    warr, carr = v_perc(trdata, rarg, earg)
    err = vtest(ttdata, warr, carr, modearg)
    for i in range(len(warr)):

      myarr = warr[i]

      mylist = [ '%.2f' % elem for elem in myarr ]

      mychar = ""

      #if (i + 1) % 2 == 0 :
      # mychar = " \\\ \n "

      print("$<$",*mylist,"$>$" ,carr[i])#,  mychar, sep=" ")
    print(" e = ", err, " for ", earg, " epochs and " , rarg, " learning rate")

  if modearg == "av":
    aarr = a_perc(trdata, rarg, earg)
    err = atest(ttdata, aarr, modearg)
    print("average: aarr = [", aarr, "] \n e = ", err, " for ", earg, " epochs and " , rarg, " learning rate")

if __name__ == "__main__":
  main()