# -*- coding: utf-8 -*-
"""hw02_stumps_bag_boost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yWO91dvTwr_0TtRUpYa2kPkarZJHMJVX
"""

#Parker Beckett u0283152
#HW01 decision trees
#!pip install scikit-learn
import math
import pandas as pd
import numpy as np
import scipy as sp
import sys #for arguments from bash
import sklearn as skl
from sklearn.tree import DecisionTreeClassifier
import random
from random import randrange
import warnings
warnings.filterwarnings("ignore")
#import warnings
#warnings.filterwarnings('ignore')
#import time #for benchmarking

#I/O file to add header line and load into dataframe
def load_csv(filename):

  attributecount = 0
  examplecount = 0

  with open(filename) as f:
    for line in f:
      examplecount += 1
      terms = line.strip().split(',')
      attributecount = len(terms)

  attributecount -= 1
  names = []

  for i in range(0,attributecount):
    name = "A"+str(i)
    names.append(name)

  names.append("label")
  df = pd.read_csv(filename, names = names)

  return df

#cleaner for unknown values -> should just treat it as an attribute -> no 'cleaner' necessary right?
def cleanunknown(data):

  newdata = data.copy()

  for col in data.columns:
    coll = data[col]
    frame = data[data[col].str.contains("unknown")]

    if (frame.shape[0] > 0):
      newval = data[col].mode()
      check = str(newval).split(" ")
      index = len(check) - 1 - 3
      totrim = check[index]
      actual = str(totrim.split("\nName")[0])

      if(actual == "unknown"):
        modelist = data[col].value_counts().index.tolist()[:2]
        actual = modelist[1]

      for i in range(len(coll)):
        coll[i] =  actual

      newdata[col] = coll

  return newdata

#cleaner for numerical values
def numtobin(data):

  newdata = data.copy()

  for col in data.columns:
    if col == "weights":
      break

    coll = data[col]

    if(coll.dtype == np.int64):
      median = coll.median()
      #loc = newdata.columns.get_loc(col)
      max = newdata[col].max()
      min = newdata[col].min()
      #print("median of ", col,": " ,median)
      #print("max of ", col,": " ,max)
      #print("min of ", col,": " ,min)

      if(median == min):
        for i in range(len(coll)):
          num = coll[i]
          result = ""

          if num > median: #check if median = max or min
            result = "greater"

          else:
            result = "lesser"

          coll[i] = result

      else:
        for i in range(len(coll)):
          num = coll[i]
          result = ""

          if num >= median: #check if median = max or min
            result = "greater"

          else:
            result = "lesser"

          coll[i] = result

      '''
      elif(median == max):
        for i in range(len(coll)):
          num = coll[i]
          result = ""

          if num >= median: #check if median = max or min
            result = "greater"

          else:
            result = "lesser"

          coll[i] = result
      '''
      '''
      for i in range(len(coll)):
        num = coll[i]
        result = ""

        if num > median: #check if median = max or min
          result = "greater"

        else:
          result = "lesser"

        coll[i] = result
      '''

      newdata[col] = coll

  return newdata

#determines attribute with most gain
def get_best(gainID, data):

  backupattr = ""
  maxentropy = 0.0
  maxattribute = "ERROR"
  rows = data.shape[0]-1
  labels = data["label"]
  uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
  labelcount = np.zeros(len(uniquelabels))

  for l in labels:
    for i in range(len(uniquelabels)):
      ul = uniquelabels[i]
      if l == ul:
        labelcount[i] += 1

  pk = labelcount / data.shape[0]
  startentropy = get_entropy(gainID, pk)
  labels = data["label"]

  for col in data.columns:
    backupattr = col
    if col == "label":
      break

    attrentropy = 0.0
    coll = data[col]
    attrvalues = coll.unique()
    attrcount = np.zeros(len(attrvalues))
    labelsbyattr = {}

    for a in range(len(coll)):
      for i in range(len(attrvalues)):
        ua = attrvalues[i]
        if coll[a] == ua:
          attrcount[i] += 1
          if(ua not in labelsbyattr):
            labelsbyattr[ua] = []
            labelsbyattr[ua].append(labels[a])

          else:
            labelsbyattr[ua].append(labels[a])

    probs = attrcount / data.shape[0]
    countsbyattr = {}

    for i in range(len(attrvalues)):
      alabels = labelsbyattr[attrvalues[i]]
      numlabels = len(alabels)
      counter = np.zeros(len(uniquelabels))
      for l in alabels:
        for u in range(len(uniquelabels)):
          ul = uniquelabels[u]
          if l == ul:
            counter[u] += 1

      countsbyattr[attrvalues[i]] = counter/numlabels

    for i in range(len(attrvalues)):
      pk = countsbyattr[attrvalues[i]]
      valentropy = get_entropy(gainID, pk)
      attrentropy += probs[i] * valentropy

    trueentropy = startentropy - attrentropy

    if trueentropy > maxentropy:
      maxentropy = trueentropy
      maxattribute = col

    attrentropy = 0.0

  if maxattribute == "ERROR":
    maxattribute = backupattr

  return maxattribute


#determines attribute with most gain for stumps
#use weights instead of just ratios for p
def get_best_stump(gainID, data, weights):#, weights): -> cant choose weights or label
  #print("STARTED GET BEST STUMP")
  attrs = []
  entropies = []

  backupattr = ""
  maxentropy = 0.0
  maxattribute = "ERROR"
  rows = data.shape[0]-1
  labels = data["label"]
  #weights = data["weights"]
  uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
  labelcount = np.zeros(len(uniquelabels))

  #use j in range (len(labels)) -> l = labels[j], w = weight[j]
  #for l in labels:
  for j in range(len(labels)):
    l = labels[j]
    for i in range(len(uniquelabels)):
      ul = uniquelabels[i]
      if l == ul:
        labelcount[i] += weights[j] #was 1 ------------------------------

  #labelcount is #samples with that label
  #pk should be sum of weights with that label

  #pk should account for weight
  #data.shape[0] is #rows
  #as of now, pk is (correct vs not)
  #should use weights instead of labelcount,

  pk = labelcount# / data.shape[0] -> / numrows

  startentropy = get_entropy(gainID, pk) #pk was originally just classification
  #labels = data["label"]

  #check entropy of each attribute -> should use weights
  for col in data.columns:
    #print("working on column: ", col)
    #backupattr = col
    if col == "weights" or col == "label":
      #print("*reached [weights] or [label]*")
      break

    backupattr = col

    attrentropy = 0.0
    coll = data[col]
    attrvalues = coll.unique()
    #print("values: ", attrvalues)
    attrcount = np.zeros(len(attrvalues))
    labelsbyattr = {}
    weightsbyattr = {} # in order?

    for a in range(len(coll)):
      for i in range(len(attrvalues)):
        ua = attrvalues[i]
        if coll[a] == ua:
          attrcount[i] += 1
          if(ua not in labelsbyattr):
            labelsbyattr[ua] = []
            labelsbyattr[ua].append(labels[a])
            weightsbyattr[ua] = []
            weightsbyattr[ua].append(weights[a])

          else:
            labelsbyattr[ua].append(labels[a])
            weightsbyattr[ua].append(weights[a])

    probs = attrcount / data.shape[0]
    countsbyattr = {}

    for i in range(len(attrvalues)):
      alabels = labelsbyattr[attrvalues[i]]
      aweights = weightsbyattr[attrvalues[i]]
      numlabels = len(alabels)
      counter = np.zeros(len(uniquelabels))
      for j in range(len(alabels)):
      #for l in alabels: -> only two, just like last time (?)
        l = alabels[j] #was i
        for u in range(len(uniquelabels)):
          ul = uniquelabels[u]
          if l == ul:
            counter[u] += aweights[j] #seems like it should work

      countsbyattr[attrvalues[i]] = counter#/numlabels #should be weightsbyattr

    #need pk to be function of weights and such

    #was "if label matches, add one to label -> / numlabels = normalized "

    for i in range(len(attrvalues)):
      pk = countsbyattr[attrvalues[i]]  #pk is probability ->  #problem here potentially -> go through code
      sum = 0
      for p in pk:
        sum += p
      pk /= sum # normalize
      #will be the same kinda?
      #print('###[no,yes]=', pk, "###")
      valentropy = get_entropy(gainID, pk)
      attrentropy += probs[i] * valentropy

    trueentropy = startentropy - attrentropy

    attrs.append(col)
    entropies.append(trueentropy)

    #print("ENTROPY: ", trueentropy)

    if trueentropy > maxentropy: #entropy is never higher than the start?
      maxentropy = trueentropy
      maxattribute = col

    attrentropy = 0.0

  '''
  #print("***returning? ", maxattribute)
  if maxattribute == "ERROR":
    #print("############################################################################################################################")
    maxattribute = backupattr
  '''

  entroparr = np.array(entropies)
  attrarr = np.array(attrs)

  p = entroparr.argsort()
  sortattrs = attrarr[p]
  sortentrs = entroparr[p]

  #print(sortentrs)

  retarr = []

  #if et.close(0.5), do next
  for i in range(len(sortattrs)):
    #print(i)
    j = len(sortattrs) - i - 1
    #print(j)
    retarr.append(sortattrs[j])

    #return sorted list -> go through attributes in order high-low

  #print("***returning: ", maxattribute)
  #return maxattribute
  return retarr

def dstump_build(gainID, data, attrindex, weights):#, weights):#, previousAttr, previousValue, currdepth):

  mytree = dtree()
  attr = [data.shape[1] - 1]
  labels = data[data.columns[data.shape[1] - 1]].unique()
  '''
  ll = data["label"]

  countyes = 0
  countno = 0

  for lll in ll:
    if lll == "yes":
      countyes+=1
    elif lll == "no":
      countno += 1

  print("starting yes labels: ", countyes)
  print("starting no labels: ", countno)
  '''
  '''
  if(len(labels) == 1):
    childs = []
    childs.append(labels[0])
    mytree.children = childs
    return mytree
  '''

  #labels are binary for this data set, but should be flexible

  '''
  if(labels.size == 1):
    mytree.label = labels[0]
    mytree.attribute = previousAttr
    mytree.value = previousValue
    return mytree
  '''

  #need way to use second best classifier if first choice is ~0.5

  bestattrs = get_best_stump(gainID, data, weights) # should not get stuck
  bestattr = bestattrs[attrindex]
  #print("bestattr: ", bestattr)
  mytree.attribute = bestattr #change if et is too high / at too close to zero
  #print("best attr to split stump based on weight: ", bestattr)
  bestvals = data[bestattr].unique() #all possible attribute values
  #print("possible values: ", bestvals)
  #print("possible labels: ", labels)
  childs = []

  for i in range(len(bestvals)):
    newtree = dtree()
    childs.append(newtree)
    #childs.append("ERROR")

  mytree.children = childs

  #each attribute value gets a branch
  limit = len(bestvals)
  for i in range(0, limit):
    #print("** i = : ", i, "**")
    #print("print twice?")

    mytree.children[i].attribute = bestattr
    bestval = bestvals[i]
    mytree.children[i].value = bestval

    #currdepth -= 1
    splitdata = data.copy()
    col = splitdata[bestattr]
    todrop = []

    for j in range(len(col)):
      if col[j] != bestval:
        todrop.append(j)

    splitdata = splitdata.drop(todrop)
    splitdata.drop(bestattr, 1)
    splitdata.reset_index(drop=True, inplace=True)

    #check out split data to find labels

    #print("DATA: ",bestattr, " = ", bestval)
    #print(splitdata.head())

    #name = str(bestattr) + "_round_" + str(i) + "_data.csv"

    #splitdata.to_csv(name)

    '''
    ll2 = splitdata["label"]

    countyes2 = 0
    countno2 = 0

    for lll2 in ll2:
      if lll2 == "yes":
        countyes2+=1
      elif lll == "no":
        countno2 += 1

    print(bestval, " yes labels: ", countyes2)
    print(bestval, " no labels: ", countno2)
    '''

    #attributes is empty, use most common label
    if(len(splitdata[bestattr])) == 0:
      uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
      labelcount = np.zeros(len(uniquelabels))

      for l in labels:
        for i in range(len(uniquelabels)):
          ul = uniquelabels[i]
          if l == ul:
            labelcount[i] += 1

      #print("NO BEST ATTRIBUTE(??)")
      common = labelcount.index(labelcount.max())
      newtree = dtree()#was mytree
      newtree.data = common
      mytree.children[i] = newtree

    #make leaf node, no recursive shenanigans
    #identify label with greatest total weight
    else:
      #print("i in else: ", i)
      #bestval = bestvals[i]
      #print("value of best attribute: ", bestval)
      newlabels = splitdata["label"]
      newweights = weights #splitdata["weights"]

      #uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
      uniquelabels = splitdata[splitdata.columns[splitdata.shape[1] - 1]].unique()
      labelcount = np.zeros(len(uniquelabels))

      for j in range(splitdata.shape[0]):
        l = newlabels[j]
        for k in range(len(uniquelabels)):
          ul = uniquelabels[k]
          if l == ul:
            labelcount[k] += newweights[k]

      #print("weightsum: ", labelcount)

      labelleaf = uniquelabels[labelcount.argmax()]

      #print("i is: ", i)

      ###print("label for ", bestattr, " = ", bestval, " : ", labelleaf)

      #print("i is: ", i)
      #print("********************************")

      #print(mytree.children)
      mytree.children[i].label = labelleaf
      #mytree.children[i].attr = bestattr
      #mytree.children[i].value = bestval
      #print("attr: ", bestattr)
      #print("attrval: ", bestvals[i]) #yes for both? i=1 twice
      #bestvals is empty

      #i is 1 twice -> why?
      #print("i: ", i)
      #mytree.children[i].label = labelleaf # i is 1 at this point ->  #oob -> index 1 outside for size 0

      #mytree.attribute = previousAttr
      #mytree.value = previousValue

      #mytree.children[i] = ID3_run(gainID, maxdepth, splitdata, bestattr, bestval, currdepth)
  '''
  print("TREE IN BUILDER")
  for i in range(len(mytree.children)):
    child = mytree.children[i]
    print("LEAF #", i)
    print("attr: ", child.attr)
    print("val: ", child.value)
    print("label: ", child.label)
  '''
  return mytree


#change so that the data has a column of weights / add new weight vector
#recursive ID3 algorithm
def ID3_run(gainID, maxdepth, data, previousAttr, previousValue, currdepth):

  mytree = dtree()
  attr = [data.shape[1] - 1]#?? / cols
  labels = data[data.columns[data.shape[1] - 1]].unique()

  if(labels.size == 1):
    mytree.label = labels[0]
    mytree.attribute = previousAttr
    mytree.value = previousValue
    return mytree

  bestattr = get_best(gainID, data)
  bestvals = data[bestattr].unique()
  childs = []

  for i in range(len(bestvals)):
    newtree = dtree()
    childs.append(newtree)

  mytree.children = childs

  for i in range(len(bestvals)):
    currdepth -= 1
    splitdata = data.copy()
    col = splitdata[bestattr]
    todrop = []

    for j in range(len(col)):
      if col[j] != bestvals[i]:
        todrop.append(j)

    splitdata = splitdata.drop(todrop)
    splitdata.drop(bestattr, 1)
    splitdata.reset_index(drop=True, inplace=True)

    if(len(splitdata[bestattr])) == 0:
      uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
      labelcount = np.zeros(len(uniquelabels))

      for l in labels:
        for i in range(len(uniquelabels)):
          ul = uniquelabels[i]
          if l == ul:
            labelcount[i] += 1

      common = labelcount.index(labelcount.max())
      newtree = mytree()
      newtree.data = common
      mytree.children[i] = newtree

    else:
      bestval = bestvals[i]
      mytree.attribute = previousAttr
      mytree.value = previousValue
      mytree.children[i] = ID3_run(gainID, maxdepth, splitdata, bestattr, bestval, currdepth)

  return mytree


#checks whether guesses are correct
def predict(tree, example):#, count):

    for child in tree.children:
      attribute = child.attribute
      value = child.value
      label = child.label
      if(attribute == ""):
        return 0
      if(example[attribute] == value):
        if label == "":
          code = predict(child, example)
          if(code == 1):
            return 1
        else:
          if(label == example["label"]):
            return 1
          else:
            return -1
    return -1

#verifies accuracy
def verifytree(tree, data, max_depth):

  count = 0
  testing = dtree()
  testing = tree
  traverse(tree, 0, 0, max_depth)

  for i in range(data.shape[0]):
    example = data.iloc[i]
    count += predict(testing, example)

  acc = count / data.shape[0]

  return acc

#traverse and prune tree, uncomment to print the tree to the console
def traverse(tree, depth, branch, max_depth):

  if tree is None:
    return
  else:
    if depth > max_depth: #prune tree
      tree.children = []
      return

    depth += 1

    #allows tree to be drawn

    space = ""
    for d in range(depth):
      space = space + "  "

    print(space + str(tree.attribute) + ">" + str(tree.value) + ": " + str(tree.label))#shouldnt be this

    for child in tree.children:
      ##print(space + "{")
      branch += 1 #allows for debugging with depth by branch
      traverse(child, depth, branch, max_depth)
      ##print(space + "}")

#dtree class
class dtree():
  def __init__(self):
    self.attribute = "" #attr
    self.value = "" #attr value
    self.label = "" #conclusion
    self.children = [] #array of children
    self.alpha = 0
    self.epsilon = 0

#calculate entropy based on heuristic
def get_entropy(gainID, pk):
  entropy = 0.0
  if (gainID == 0): #normal gain
    for p in pk:
      term = 0
      if p != 0:
        term = p * np.log2(p)
      entropy -= term
    return entropy
  if(gainID == 1): #majority error
    majerr = 1 - pk.max()
    for p in pk:
      entropy += p * majerr
    return entropy
  if(gainID == 2): #gini
    for p in pk:
      entropy += p * p
    return 1 - entropy

def adaboost(rounds, data, gainID):#, votes):
###def adaboost(rounds, dataX, dataY, gainID):#, votes):
  models = []
  alphas = []
  elems = data.shape[0]

  #weights = data["weights"]


  weights = []
  for i in range(elems):
    weights.append(1 / elems)

  ###elems = dataX.shape[0]

  predictions = []
  for i in range(rounds):

    pred = []
    notfound = True
    ind = 0

    et = 0
    at = 0
    stump = None

    while(notfound):

      ###weights = dataX["weights"]

      #find stump, check <et>, use next best if bad classifier

      stump = dstump_build(gainID, data, ind, weights) #index into best attribute

      #check bestattr

      ###teststump = DecisionTreeClassifier(max_depth = 1)

      ###dataX = dataX.drop(["weights"], 1)
      ###dataX.reset_index(drop=True, inplace=True)
      ###teststump.fit(dataX, dataY, sample_weight = weights)

      ###print(skl.tree.export_text(teststump))

      #print("ROUND ", i)
      #print("building stump on: ", stump.attribute)

      #weights = data["weights"]
      sum = 0
      for j in range(elems): #must go through entire data set multiple times
        #print("j: ", j)
        sample = data.iloc[j]
        ###sample = dataX.iloc[j]
        actual = sample["label"]
        ###actual = dataY.iloc[j]
        #print("sample: ", sample)
        hx = stump_predict(stump, sample)
        ###hx = teststump.predict(sample.values.reshape(1, -1))
        pred.append(hx)
        if(not hx == actual): #weights increase when not correctly classified -> epsilon approaches 50%
          sum += weights[j]

      predictions.append(pred) #predictions of each sample? for each round

      et = sum # = 0.1192 at first -> is increasing (596 misclassified at first)

      #print("ET: ", et) # = combined weight of misclassified -> should be 0.0002 * 596

      at = 0.5*np.log((1-et)/et)

      #print("AT: ", at) # = 1

      if np.isclose(at, 0.0) or at <= 0.0:
        ind += 1
        #data has labels, weights as well
        if(ind == data.shape[1] - 2): #built on best attribute -> if no other attribute works, use bad one
          #ind = 0
          notfound = False

          #or just return when options start getting exhausted -> ~36 until index was too much

          #just use whatever you have now -> or start over? after a while, only so many stumps
          '''
          stump = dstump_build(gainID, data, ind) #index into best attribute

          ###teststump = DecisionTreeClassifier(max_depth = 1)

          ###dataX = dataX.drop(["weights"], 1)
          ###dataX.reset_index(drop=True, inplace=True)
          ###teststump.fit(dataX, dataY, sample_weight = weights)

          ###print(skl.tree.export_text(teststump))

          #print("ROUND ", i)
          #print("building stump on: ", stump.attribute)

          weights = data["weights"]
          sum = 0
          for j in range(elems):
            #print("j: ", j)
            sample = data.iloc[j]
            ###sample = dataX.iloc[j]
            actual = sample["label"]
            ###actual = dataY.iloc[j]
            #print("sample: ", sample)
            hx = stump_predict(stump, sample)
            ###hx = teststump.predict(sample.values.reshape(1, -1))
            pred.append(hx)
            if(not hx == actual): #weights increase when not correctly classified -> epsilon approaches 50%
              sum += weights[j]

          predictions.append(pred) #predictions of each sample? for each round

          et = sum # = 0.1192 at first -> is increasing (596 misclassified at first)

          #print("ET: ", et) # = combined weight of misclassified -> should be 0.0002 * 596

          at = 0.5*np.log((1-et)/et)
          '''

      else:
        #is slowing down the model, but necessary
        notfound = False

    alphas.append(at)

    stump.alpha = at

    stump.epsilon = et

    models.append(stump)

    ###models.append(teststump)

    #traverse(stump, 0, 0, 2)

    for xx in range(elems):
      sample = data.iloc[xx]
      yi = sample["label"]
      hx = stump_predict(stump, sample)
      ###sample = dataX.iloc[xx]
      ###yi = dataY.iloc[xx]
      ###hx = teststump.predict(sample.values.reshape(1, -1))
      #print("actual: ", yi)
      #print("predicted: ", hx)
      weight = weights[xx]
      power = -at * hx * yi
      #print("power: ", power)
      '''
      if(power > 0):
        print("-at: ", -at)
        print("hx: ", hx)
        print("yi: ", yi)
        print("power: ", power)
      '''

      wmid = weight * np.exp(power)
      weights[xx] = wmid

    #print("WEIGHTS:", weights)

    norm = np.sum(weights)
    weights /= norm

    #check = np.sum(weights)

    #print(weights)

    #print("1 ?= ", check)

    #print("WEIGHTS: \n", weights)

    #print("pre-drop")
    #print(data.head())

    '''
    data = data.drop(["weights"], 1)
    data.reset_index(drop=True, inplace=True)

    #print("post-drop")

    #print(data.head())
    #print("pre-insert")

    data.insert(data.shape[1] - 1, "weights", weights)
    ###dataX.insert(dataX.shape[1], "weights", weights)
    '''

    ###print(dataX.head())
    ###print(dataY.head())

    #print(data.head())

    #print("post-insert")

    #h(t) should be -1 or 1

    #set weight column equal to new weights

    #print("round ", i, " complete")

  #need to return hypothsesis

  #athtx = np.zeros(elems)
  #HofX =

  ###return (alphas, models)#tvotes #?
  return models

def stump_predict(stump, sample):#compare label to stump prediction

  attr = stump.attribute
  #print("attribute: ", attr)
  actual = sample["label"]
  value = sample[attr]
  #print("value: ", value)
  guess = ""
  #print("actual: ", actual)

  for c in stump.children:
    if c.value == value:
      guess = c.label
      '''
      print("attr: ", attr)
      print("value: ", value)
      print("guess: ", guess)
      '''

  #print("guess: ", guess)

  #if guess == actual:
    #return 1

  #print("misclassified!")
  #return -1

  #guess will be -1 sometimes
  '''
  if(guess < 0):
    print("GUESSED YES!, children:")
    #print(attr)
    #print(value)
    #print(guess)
    #traverse(stump, 0, 0, 2)


    for c in stump.children:
      print(c.attribute)
      print(c.value)
      print(c.label)


    x = 1/0
  '''

  return guess

def evaluatemodels(models, data, stumperr):
###def evaluatemodels(alphas, models, dataX, dataY):
  #print("alphas: ", alphas)
  #print("models: ", models)
  datasize = data.shape[0]
  ###datasize = dataY.shape[0]
  miss = 0
  for i in range(datasize):
    sample = data.iloc[i]
    ###sample = dataX.iloc[i]
    actual = sample["label"]
    ###actual = dataY.iloc[i]

    hypoth = []
    for k in range(len(models)):
      model = models[k]
      ###alpha = alphas[k]
      stumperr.append(model.epsilon)
      guess = stump_predict(model, sample)
      ###guess = model.predict(sample.values.reshape(1, -1))
      hypoth.append(guess*model.alpha)
      ###hypoth.append(guess*alpha)
    final = np.sum(hypoth)
    if(final > 0):
      final = 1
    else:
      final = -1

    if not final == actual:
      miss += 1
      #print("miss at i = ", i)
      #print("guessed: ", final, "wanted: ", actual)

  return (datasize - miss)/datasize

def encode(datain):
  data = datain.copy()
  for (attr, values) in data.iteritems():
    if attr == "weights" or attr == "label":
      #print("found weights or label, stopping encoding")
      #break
      vec = data[attr]
      data = data.drop([attr], 1)
      data.reset_index(drop=True, inplace=True)
      data.insert(data.shape[1], attr, vec)

    else:
      #print(attr)
      holder = np.zeros(data.shape[0])
      uniques = data[attr].unique()
      #print("unique: ", uniques)

      for i in range(len(values)):
        val = list(uniques).index(values[i])
        #print("val: ", val)
        holder[i] = val

      #print(holder)
      data = data.drop([attr], 1)
      data.reset_index(drop=True, inplace=True)

      data.insert(data.shape[1], attr, holder)
      #this ends up with weird shaped table

    #x = 1/0
  return data


# Create a random subsample from the dataset with replacement
def subsample(data):#, ratio=1.0): #take m' examples from dataset
 #print(data.head())
 sample = []
 size = data.shape[0]
 #n_sample = round(size)# * ratio)
 while len(sample) < size: #n_sample:
  index = randrange(size)
  sample.append(data.iloc[index])
 return sample

def learnRF(gainID, maxdepth, data, previousAttr, previousValue, currdepth):
    #print("starting learnRF")
    #print(data.head()) #should not have weights
    mytree = dtree()
    attrcount = data.shape[1] - 1  # no weights
    #print("attrcount: ", attrcount)
    labels = data[data.columns[data.shape[1] - 1]].unique()

    #print(data.head()) # has weights -> ????

    if (labels.size == 1):
        mytree.label = labels[0]
        mytree.attribute = previousAttr #is label??
        mytree.value = previousValue
        #print("found leaf node: ", mytree.attribute, "->", mytree.value, "=", mytree.label)
        return mytree

    sizes = [2, 4, 6]

    sizeind = randrange(3)

    while sizes[sizeind] > attrcount and attrcount > 2:
        sizeind = randrange(3)
        #print("size of features to lose: ", sizes[sizeind])

    numfeatures = sizes[sizeind]

    #print("features to eliminate: ", numfeatures)

    findices = []
    dups = set()

    while len(findices) < numfeatures and numfeatures > 2:
        findex = randrange(attrcount)
        #print("feature index: ", findex)
        if findex not in dups:
            findices.append(findex)

    #newdata is only some features

    newdata = data.copy()

    newdata = newdata.drop(newdata.columns[findices], 1)
    newdata.reset_index(drop=True, inplace=True)

    #print("data should be eliminated...")
    #print(newdata.head())

    bestattr = get_best(gainID, newdata)
    if bestattr == "label":
        # newtree = dtree()
        # newtree.label = "****"
        return None  # newtree#None #?? was mytree -> make labels go away
        # x = 1/0
    bestvals = data[bestattr].unique()
    childs = []

    for i in range(len(bestvals)):
        newtree = dtree()
        childs.append(newtree)

    mytree.children = childs

    for i in range(len(bestvals)):
        currdepth -= 1
        splitdata = newdata.copy()
        col = splitdata[bestattr]
        todrop = []

        for j in range(len(col)):
            if col[j] != bestvals[i]:
                todrop.append(j)

        splitdata = splitdata.drop(todrop)
        splitdata.drop(bestattr, 1)
        splitdata.reset_index(drop=True, inplace=True)

        if (len(splitdata[bestattr])) == 0:
            uniquelabels = data[data.columns[data.shape[1] - 1]].unique()
            labelcount = np.zeros(len(uniquelabels))

            for l in labels:
                for i in range(len(uniquelabels)):
                    ul = uniquelabels[i]
                    if l == ul:
                        labelcount[i] += 1

            common = labelcount.index(labelcount.max())
            newtree = dtree()
            newtree.label = common
            mytree.children[i] = newtree
            #print("reached end of branch")

        else:
            bestval = bestvals[i]
            mytree.attribute = previousAttr #
            #print(previousAttr)
            mytree.value = previousValue
            #print(previousValue)
            #can't split on label -> whatever
            mytree.children[i] = learnRF(gainID, maxdepth, splitdata, bestattr, bestval, currdepth)

    #print("made one tree")
    return mytree


def makerandomforest(gainID, data, T):
    trees = []
    for i in range(T):
        #print("IN RANDOM FOREST")
        #print(data.head()) #does not have weights -> check out subsample,
        sample = subsample(data)
        #print(sample[0])
        #in arrstodf?
        sampledf = arrstodf(sample)

        #need weights to go away -> ??
        #print("making tree...")
        #print(sampledf.head())
        mytree = learnRF(gainID, 128, sampledf, "root", "none", 0)
        #print("BUILT TREE")

        traverse(mytree, 0, 0, 128)

        #similar, but choose random subset before splitting

        #mytree = ID3_run(gainID, 128, sampledf, "root", "none", 0)  # taking a very long time
        trees.append(mytree)
        #if (i % 25 == 0):
        #   print(i)

    return trees

def makerandomforest(gainID, data, T):
    trees = []
    for i in range(T):
        #print("IN RANDOM FOREST")
        #print(data.head()) #does not have weights -> check out subsample,
        sample = subsample(data)
        #print(sample[0])
        #in arrstodf?
        sampledf = arrstodf(sample)

        #need weights to go away -> ??
        #print("making tree...")
        #print(sampledf.head())
        mytree = learnRF(gainID, 128, sampledf, "root", "none", 0)
        print("BUILT TREE")

        traverse(mytree, 0, 0, 128)

        #similar, but choose random subset before splitting

        #mytree = ID3_run(gainID, 128, sampledf, "root", "none", 0)  # taking a very long time
        trees.append(mytree)
        if (i % 25 == 0):
            print(i)

    return trees

def makebaggedtrees(gainID, data, T):
  trees = []
  for i in range(T):
    sample = arrstodf(subsample(data))
    mytree = ID3_run(gainID, 128, sample, "root", "none", 0)
    trees.append(mytree)
  #def ID3_run(gainID, maxdepth, data, previousAttr, previousValue, currdepth):
  #

  return trees

def makebaggedtreesexp(gainID, data, T): # special case when subsample already happened
  trees = []
  for i in range(T):
    #sample = arrstodf(subsample(data))
    mytree = ID3_run(gainID, 128, data, "root", "none", 0)
    trees.append(mytree)
  #def ID3_run(gainID, maxdepth, data, previousAttr, previousValue, currdepth):
  #

  return trees

def arrstodf(dataarr):
  check = dataarr[0]
  colnames = []
  for i in range(len(check) - 1):
    name = "A" + str(i)
    colnames.append(name)
  #colnames.append("weights")
  colnames.append("label")

  data = np.array(dataarr)
  df = pd.DataFrame(data = data, columns = colnames)
  ##print(df.head())
  return df

def testbaggedtrees(trees, data):
  #guesses = []
  miss = 0
  labels = data["label"]
  for i in range(data.shape[0]):
    agg = 0
    for j in range(len(trees)):
      tree = trees[j]
      sample = data.iloc[i]
      agg += makeprediction(tree, sample)
    guess = agg / len(trees)
    if guess > 0:
      guess = 1
    else:
      guess = -1
    if(not guess == labels[i]):
      miss += 1
    #guesses.append(guess)
  #return guesses
  return (data.shape[0] - miss)/data.shape[0]

#follow tree to label
def makeprediction(tree, example):#, count):
    ret = 0.0
    #list has no attr children -> 'tree' is a list? ->
    for child in tree.children:
      if child is None:
        print("end of tree")
      else:
          attribute = child.attribute
          value = child.value
          label = child.label
          if(attribute == ""):
            return 0
          if(example[attribute] == value):
            if label == "":
              ret = makeprediction(child, example)
            else:
              return label
    return ret

###############################################################################################################

#makes one update to the weight vector for every pass over the data
def batchGD(traindata, testdata):
    print("BATCH GD")
    #shuffled = traindata.sample(frac=1)
    shuffled = traindata
    weight = np.zeros(traindata.shape[1] - 1) #problems elsewhere?
    bias = 0
    grad_bias = 0
    grad_weight = np.zeros(traindata.shape[1] - 1)
    elems = shuffled.shape[0]

    r = 0.000015#/elems -> got close, but still up and down

    #print(elems) # = 53

    #yvals = traindata[traindata.columns[traindata.shape[1]-1]]
    #xvals = traindata.drop()

    #take average of gradients of all training examples, use mean gradient to update parameters
    #find gradient of each sample

    threshold = 10^-6


    '''for rr in range(100, 1000, 10):
        eps = 1
        iter = 0
        r = 1/rr'''

    eps = 1
    iter = 0


    while eps > threshold:
            #if (iter > 10000):
            #    print("overiter")
            #    break
            iter += 1
            #if iter % 1000 == 0:
                #print(iter)
            for i in range(elems):
                sampled = shuffled.iloc[i].values.flatten().tolist()
                #print(sampled)
                yi = sampled[len(sampled) - 1]
                xi = sampled[:-1]
                #xi.append(1)#for bias
                #weight.append(0)
                #print(yi)
                #print(xi)

                for j in range(len(xi)):
                    wtx = weight.transpose() @ xi
                    #print("wtx: ", wtx)
                    grad_bias -= (yi - wtx - bias) * 1
                    grad_weight[j] -= (yi - wtx - bias) * xi[j] #sums up all elements
                    #sum up (yi - wtxi)xij -> xij => all the x1s, x2s, etc.
                    #d/dwj =

            #loop is done, can change stuff
            bias_avg = grad_bias / elems
            weight_avg = grad_weight / elems

            #print(bias_avg)
            #print(weight_avg)

            oldb = bias
            oldw = weight

            bias = oldb - r*bias_avg
            weight = oldw - r*weight_avg

            diff = oldw - weight

            eps = np.linalg.norm(diff)

            print("epsilon: ", eps)
            print("weight: ", weight)
            #print("bias: ", bias) # gets enormous and negative

    print("out of the loop")

    if eps < threshold:
                print("converged")
                print(weight)
                #return weight
    else:
                print("itered out with r = ", r, ", eps = ", eps)


    #very small r is close  -> 0.05

    return 0

#
def stochGD(traindata, testdata):
    print("SGD")
    shuffled = traindata
    weight = np.zeros(traindata.shape[1] - 1)  # problems elsewhere?
    bias = 0
    grad_bias = 0
    grad_weight = np.zeros(traindata.shape[1] - 1)
    elems = shuffled.shape[0]

    old = weight

    r = 0.05

    for l in range(0, 50):

        r += (1/100)

        for i in range(elems):
            bias = 0
            weight = np.zeros(traindata.shape[1] - 1)  # problems elsewhere?
            sampled = shuffled.iloc[i].values.flatten().tolist()
            # print(sampled)
            yi = sampled[len(sampled) - 1]
            xi = sampled[:-1]
            # xi.append(1)#for bias
            # weight.append(0)
            #print(yi)
            #print(xi)

            wtx = weight.transpose() @ xi

            #for j in range(len(xi)):

            #print("wtx: ", wtx)
            bias = bias + r * (yi - wtx - bias)
            for j in range(len(weight)):
                weight[j] = weight[j] + r * (yi - wtx - bias) * xi[j]

            diff = old - weight

            eps = np.linalg.norm(diff)

            old = weight

        #print("r: ", r)
        #print("weight: ", weight)
        #print("bias: ", bias)

            # sum up (yi - wtxi)xij -> xij => all the x1s, x2s, etc.
            # d/dwj =
        print("r: ", r)
        print("weight: ", weight)
        print("bias: ", bias)
        '''
        oldb = bias
        oldw = weight

        bias = oldb - grad_bias * r
        weight = oldw - grad_weight * r

        diff = oldw - weight

        eps = np.linalg.norm(diff)
        '''

        #print("epsilon: ", eps)
        #print("weight: ", weight)

    return 0

def calcideal(traindata, testdata):
  Y = traindata[traindata.columns[traindata.shape[1]-1]]
  y = Y.values
  X = traindata.drop(traindata.columns[0], axis = 1)
  x = X.values

  #print(y)
  #print(x)
  #singular matrix?
  xxt = np.matmul(x.transpose(), x)
  print(xxt) #invertible
  invstep = np.linalg.inv(xxt) #singular matrix if wrong way
  #size 5000 diff from size 16
  print(invstep)
  print(x.shape)
  print(y.shape)
  xystep = np.matmul(y, x)
  midstep = np.matmul(x, invstep)#, x)
  print(invstep.shape)
  print(xystep.shape)
  other = np.matmul(y, midstep)
  theta = np.matmul(xystep, invstep)  #single matrix?

  print("theta: ", other)

  return 0
###############################################################################################################
def testEXP(traindata, testdata, gainarg, T):
  sampledf = None
  print("tree variance stuff")
  treecollection = []
  treeboquet = []
  for j in range(5):
    for i in range(5):
      samples = []
      dups = set()
      while (len(dups) < 1000):
        index = randrange(traindata.shape[0])
        if not index in dups:
          dups.add(index)
          samples.append(traindata.iloc[index])
      #print("1000 samples taken")
      # have 1000 samples
      # now how to make array of samples into dataframe again?
      #print("about to convert to df")
      sampledf = arrstodf(samples)  # -> not real
      #print("converted to df")
      #print(sampledf.head())
      trees = makebaggedtreesexp(gainarg, sampledf, 5)  # was 500,
      #print("500 trees made")
      treeboquet.append(trees)
    treecollection.append(treeboquet)

  avg = 0
  preds = []
  acts = []
  for treeboquet in treecollection:
    tree = treeboquet[0][0]
    for i in range(10):#sampledf
      pred = makeprediction(tree, sampledf.iloc[i])
      actual = sampledf["label"].iloc[i]
      avg += pred
      preds.append(pred)
      acts.append(actual)

  #print(preds)
  #print(acts)

  avg /= 50
  #print("avg: ", avg)
  var = 0
  for i in range(len(preds)):
    var += (avg - pred)**2
  vvar = math.sqrt(var)
  print("var: ",vvar)
  #mean square deviation


  # still need to get data

  # print("trainerr: ", testerr)
  # print("testerr: ", trainerr)

  # print("weights: ", len(weights))

  # tree = ID3_run(gainarg, deptharg, traindata, "root", "none", 0)

  # acc = adaboost(stump, weights)

  # acc = verifytree(tree, testdata, deptharg)

  # print("tree with depth " + str(deptharg) + " and gain code " + str(gainarg) + " trained on " + trainarg + " and tested on " + testarg + " has accuracy : " + str(acc))

def teststump(traindata, testdata, gainarg, T):
  stumptrainerr = []
  stumptesterr = []
  trainerr = []
  testerr = []
  iters = []

  errtrain = "totaltrain.csv"
  errtest = "totaltest.csv"
  stumptrain = "stumptrain.csv"
  stumptest = "stumptest.csv"

  # algorithm is too slow for 500 -> do 25? see on local
  for i in range(T): #did 50, no way
    rounds = 5*(i+1)  # built-in doesnt hone in on a6
    iters.append(rounds)
    # rounds = 10

    votes = np.zeros(traindata.shape[0])

    # print(testdataY.head())

    # x = 1/0

    models = adaboost(rounds, traindata, gainarg)  # , votes)
    ###(alphas, models) = adaboost(rounds, traindataX, traindataY, gainarg)#, votes)

    #print(rounds, " rounds complete, made models, evaluating...")

    acctrain = evaluatemodels(models, traindata, stumptrainerr)
    acctest = evaluatemodels(models, testdata, stumptesterr)

    trainerr.append(acctrain)
    testerr.append(acctest)

    # stump is decent enough ->

    ###acc = evaluatemodels(alphas, models, testdataX, testdataY)

    print("train: ", acctrain)
    print("test: ", acctest)

      #can it work fast enough?

  #np.savetxt(errtrain, trainerr, delimiter=', ')
  #np.savetxt(errtest, testerr, delimiter=', ')
  #np.savetxt(stumptrain, stumptrainerr, delimiter=', ')
  #np.savetxt(stumptest, stumptesterr, delimiter=', ')
  #np.savetxt("iterations.csv", iters, delimiter=', ')

def testbag(traindata, testdata, gainarg, T):
#stumptrainerr = []
  #stumptesterr = []
  trainerr = []
  testerr = []
  iters = []

  errtrain = "totaltreetrain.csv"
  errtest = "totaltreetest.csv"
  #stumptrain = "stumptrain.csv"
  #stumptest = "stumptest.csv"

  #algorithm is too slow for 500 -> do 5 - 50
  for i in range(T):
    rounds = 5*(i+1) #built-in doesnt hone in on a6
    iters.append(rounds)
    #rounds = 10

    #votes = np.zeros(traindata.shape[0])

    trees = makebaggedtrees(gainarg, traindata, rounds)
    acctrain = testbaggedtrees(trees, traindata)
    acctest = testbaggedtrees(trees, testdata)

    #print(testdataY.head())

    #x = 1/0

    #models = adaboost(rounds, traindata, gainarg)#, votes)
    ###(alphas, models) = adaboost(rounds, traindataX, traindataY, gainarg)#, votes)

    #print(rounds, " rounds complete, made models, evaluating...")

    #acctrain = evaluatemodels(models, traindata, stumptrainerr)
    #acctest = evaluatemodels(models, testdata, stumptesterr)

    trainerr.append(acctrain)
    testerr.append(acctest)



    ###acc = evaluatemodels(alphas, models, testdataX, testdataY)

    #print("train: ", acctrain)
    #print("test: ", acctest)

  #np.savetxt(errtrain, trainerr, delimiter = ', ')
  #np.savetxt(errtest, testerr, delimiter = ', ')
  #np.savetext(stumptrain, stumptrainerr, delimiter = ', ')
  #np.savetext(stumptest, stumptesterr, delimiter = ', ')
  #np.savetxt("treeiterations.csv", iters, delimiter = ', ')

def testRF(traindata, testdata, gainarg, T):
  #RANDOM FOREST:

  # stumptrainerr = []
  # stumptesterr = []
  trainerr = []
  testerr = []
  iters = []

  errtrain = "rforesttrain.csv"
  errtest = "rforesttest.csv"
  # stumptrain = "stumptrain.csv"
  # stumptest = "stumptest.csv"

  print("attempting random forest")

  # algorithm is too slow for 500 -> do 5 - 50
  for i in range(T):
      rounds = 5 * (i + 1)
      iters.append(rounds)
      # rounds = 10

      # votes = np.zeros(traindata.shape[0])

      # print("DATA HAS WEIGHTS?")
      # print(traindata.head()) #has no weights -> mrf

      trees = makerandomforest(gainarg, traindata, rounds)  # has weights here ???

      print("made forest")

      acctrain = testbaggedtrees(trees, traindata)
      acctest = testbaggedtrees(trees, testdata)

      print("acctrain: ", acctrain)
      print("acctest: ", acctest)

      # print(testdataY.head())

      # x = 1/0

      # models = adaboost(rounds, traindata, gainarg)#, votes)
      ###(alphas, models) = adaboost(rounds, traindataX, traindataY, gainarg)#, votes)

      print(rounds, " rounds complete, made models, evaluating...")

      # acctrain = evaluatemodels(models, traindata, stumptrainerr)
      # acctest = evaluatemodels(models, testdata, stumptesterr)

      trainerr.append(acctrain)
      testerr.append(acctest)
      testerr.append(acctest)

      ###acc = evaluatemodels(alphas, models, testdataX, testdataY)

      # print("train: ", acctrain)
      # print("test: ", acctest)

  #np.savetxt(errtrain, trainerr, delimiter=', ')
  #np.savetxt(errtest, testerr, delimiter=', ')
  # np.savetext(stumptrain, stumptrainerr, delimiter = ', ')
  # np.savetext(stumptest, stumptesterr, delimiter = ', ')
  ###np.savetext("treeiterations.csv", iters, delimiter = ', ')

def main():
  #DATA PROFILE:
  #bank client data
  #age, job, marital, education, default?, balance, housing, loan
  #campaign data
  #contact, day, month, duration
  #others
  #currentcontacts, pastdays, previouscontacts, previousoutcome,
  # -> subscribed to term deposit?

  #dstump_build()

  if(len(sys.argv)< 4):
    print("the arguments should be <gain>, <T>, <training>, <testing>, <type>")

  #gainarg = int(sys.argv[1])
  #targ = int(sys.argv[2])
  #trainarg =  str(sys.argv[3])
  #testarg = str(sys.argv[4])
  #typearg = str(sys.argv[5]



  gainarg = 0
  deptharg = 128
  targ = 20

  typearg = 0#need to switch this

  traindataGD = load_csv("train_c.csv")
  testdataGD = load_csv("test_c.csv")

  batchGD(traindataGD, testdataGD)
  stochGD(traindataGD, testdataGD)
  #calcideal(traindataGD, testdataGD)

  trainarg = "train.csv"
  testarg = "test.csv"

  traindata = numtobin(load_csv(trainarg))
  testdata = numtobin(load_csv(testarg))

  #unknown is treated as attribute value, so no cleaner needed
  #traindata = cleanunknown(traindata)
  #testdata = cleanunknown(testdata)

  weights = np.zeros(traindata.shape[0])
  nlabels = np.zeros(traindata.shape[0])
  labels = traindata["label"]


  size = len(weights)

  uniques = traindata[traindata.columns[traindata.shape[1] - 1]].unique()

  #makes binary labels more clear
  for pp in range(size):
    #for u in uniques: #WILL BE BINARY
    if labels[pp] == uniques[0]:
      nlabels[pp] = 1
    else:
      nlabels[pp] = -1

  for i in range(0, size):
    weights[i] = 1/size

  traindata = traindata.drop(["label"], 1)
  traindata.reset_index(drop=True, inplace=True)

  traindata.insert(traindata.shape[1], "label", nlabels)

  #traindata.insert(traindata.shape[1] - 1, "weights", weights)
  #testdata.insert(traindata.shape[1] - 1, "weights", weights)

  #encoding helps test with sklearn
  traindata = encode(traindata)
  testdata = encode(testdata)

  #print(traindata.head())
  #print(testdata.head())

  #traindata.to_csv("mid.csv")

  #weights = np.zeros(testdata.shape[0])
  nlabels = np.zeros(testdata.shape[0])
  labels = testdata["label"]

  #uniques = testdata[testdata.columns[testdata.shape[1] - 1]].unique() -> both have same labels

  for pp in range(testdata.shape[0]):
    #for u in uniques: #WILL BE BINARY
    if labels[pp] == uniques[0]:
      nlabels[pp] = 1
    else:
      nlabels[pp] = -1

  testdata = testdata.drop(["label"], 1)
  testdata.reset_index(drop=True, inplace=True)

  testdata.insert(testdata.shape[1], "label", nlabels)

  typearg = 4
  targ = 2

  trdata = load_csv("train_c.csv")
  ttdata = load_csv("test_c.csv")

  '''
  if(typearg == 0):
    teststump(traindata, testdata, gainarg, targ)
  elif(typearg == 1):
    testbag(traindata, testdata, gainarg, targ)
  elif(typearg == 2):
    testRF(traindata, testdata, gainarg, targ)
  elif(typearg == 3):
    testEXP(traindata, testdata, gainarg, targ)
  elif(typearg == 4):
    calcideal(trdata, ttdata)
    #remove later
  '''
if __name__=="__main__":
  main()

